{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "point_cloud_path = 'data/shapenet_pc.npz'\n",
    "annotations_path = 'data/shapnet_tokenized.npy'\n",
    "\n",
    "annotations = np.load(annotations_path, allow_pickle=True).item()\n",
    "point_clouds = np.load(point_cloud_path, allow_pickle=True)['arr_0']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T15:38:45.861487Z",
     "start_time": "2023-11-17T15:38:03.498945Z"
    }
   },
   "id": "4735a6a0325df956"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['taxonomy_map', 'tokenized_taxonomy'])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.keys() ## dict_keys(['taxonomy_map', 'tokenized_taxonomy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T15:58:23.948511Z",
     "start_time": "2023-11-17T15:58:23.918848Z"
    }
   },
   "id": "c6185a52fd5edf94"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "pcs = {k: torch.tensor(v.vertices) for k, v in point_clouds.item().items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T15:53:11.783489Z",
     "start_time": "2023-11-17T15:53:04.252865Z"
    }
   },
   "id": "bb8cee2e6535b68f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6727b0fbb28dcd88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "{item['id']: annotations['tokenized_taxonomy'][item['category']]['tokens'] for item in annotations['taxonomy_map']}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a8e620ead01e5f5"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import copy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import numpy as np\n",
    "\n",
    "class ShapeNetCore(Dataset):\n",
    "\n",
    "    def __init__(self, path, annotations_path, split, scale_mode, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert split in ('train', 'val', 'test')\n",
    "        assert scale_mode is None or scale_mode in ('global_unit', 'shape_unit', 'shape_bbox', 'shape_half', 'shape_34', 'unit_sphere')\n",
    "        self.path = path\n",
    "        self.annotations_path = annotations_path\n",
    "\n",
    "        self.split = split\n",
    "        self.scale_mode = scale_mode\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pointclouds = []\n",
    "\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "\n",
    "        annotations = np.load(self.annotations_path, allow_pickle=True).item()  ## dict_keys(['taxonomy_map', 'tokenized_taxonomy'])\n",
    "\n",
    "        ann_map = {item['id']: annotations['tokenized_taxonomy'][item['category']]['tokens'] for item in annotations['taxonomy_map']}\n",
    "\n",
    "        def map_pc(pc_id, pc):\n",
    "            pc, shift, scale = self.scale_pc(torch.tensor(pc.vertices, dtype=torch.float32))\n",
    "            return {\n",
    "                'pointcloud': pc,\n",
    "                'latent_text': ann_map[pc_id],\n",
    "                'id': pc_id,\n",
    "                'shift': shift,\n",
    "                'scale': scale\n",
    "            }\n",
    "        \n",
    "        point_clouds = np.load(self.path, allow_pickle=True)['arr_0'].item()\n",
    "        self.pointclouds = [map_pc(id, pc) for id, pc in point_clouds.items()]\n",
    "        \n",
    "\n",
    "        # Deterministically shuffle the dataset\n",
    "        self.pointclouds.sort(key=lambda data: data['id'], reverse=False)\n",
    "        random.Random(2023).shuffle(self.pointclouds)\n",
    "\n",
    "        generator = torch.Generator().manual_seed(42)\n",
    "        split_data = random_split(self.pointclouds, [0.85, 0.15], generator=generator)\n",
    "\n",
    "        if self.split is not None:\n",
    "            if self.split == 'train':\n",
    "                self.pointclouds = list(split_data[0])\n",
    "            if self.split == 'val':\n",
    "                self.pointclouds = list(split_data[1])\n",
    "\n",
    "    def scale_pc(self, pc):\n",
    "        if self.scale_mode == 'shape_unit':\n",
    "            shift = pc.mean(dim=0).reshape(1, 3)\n",
    "            scale = pc.flatten().std().reshape(1, 1)\n",
    "        elif self.scale_mode == 'shape_half':\n",
    "            shift = pc.mean(dim=0).reshape(1, 3)\n",
    "            scale = pc.flatten().std().reshape(1, 1) / (0.5)\n",
    "        elif self.scale_mode == 'shape_34':\n",
    "            shift = pc.mean(dim=0).reshape(1, 3)\n",
    "            scale = pc.flatten().std().reshape(1, 1) / (0.75)\n",
    "        elif self.scale_mode == 'shape_bbox':\n",
    "            pc_max, _ = pc.max(dim=0, keepdim=True)  # (1, 3)\n",
    "            pc_min, _ = pc.min(dim=0, keepdim=True)  # (1, 3)\n",
    "            shift = ((pc_min + pc_max) / 2).view(1, 3)\n",
    "            scale = (pc_max - pc_min).max().reshape(1, 1) / 2\n",
    "        elif self.scale_mode == 'unit_sphere':\n",
    "            # Shift to the center of the point cloud\n",
    "            shift = pc.mean(dim=0).reshape(1, 3)\n",
    "\n",
    "            # Scale to fit into a unit sphere\n",
    "            max_dist = torch.sqrt(((pc - shift) ** 2).sum(dim=1)).max()\n",
    "            scale =  max_dist / torch.ones([1, 1])\n",
    "        else:\n",
    "            shift = torch.zeros([1, 3])\n",
    "            scale = torch.ones([1, 1])\n",
    "\n",
    "        return (pc - shift) / scale, shift, scale\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pointclouds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = {k:v.clone() if isinstance(v, torch.Tensor) else copy(v) for k, v in self.pointclouds[idx].items()}\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return data\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:16:35.256356Z",
     "start_time": "2023-11-17T16:16:34.136747Z"
    }
   },
   "id": "59194e94f81d2597"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dataset = ShapeNetCore(\n",
    "    path = 'data/shapenet_pc.npz', \n",
    "    annotations_path = 'data/shapnet_tokenized.npy',\n",
    "    split = 'train',\n",
    "    scale_mode = 'unit_sphere',\n",
    "    transform=None\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:17:33.290220Z",
     "start_time": "2023-11-17T16:16:35.644250Z"
    }
   },
   "id": "bd61b9b82c9e6dc5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "43528"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:17:33.322707Z",
     "start_time": "2023-11-17T16:17:33.296853Z"
    }
   },
   "id": "d7f0ac287863d826"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
