{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T19:57:12.944373Z",
     "start_time": "2023-11-09T19:57:12.928403Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils.objaverse_dataset import ObjaversePointCloudDataset\n",
    "\n",
    "annotations_file = '/Users/kostyalbalint/Documents/Egyetem/7.Felev/Szakdolgozat/objaverse_labeling/concatenated_annotations.npy'\n",
    "pc_dir = '/Users/kostyalbalint/Documents/Egyetem/7.Felev/Szakdolgozat/pointClouds3000'\n",
    "\n",
    "dataset = ObjaversePointCloudDataset(annotations_file=annotations_file, pc_dir=pc_dir, file_ext='.npz', scale_mode='shape_bbox', load_to_mem=False, transform=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T19:57:22.151514Z",
     "start_time": "2023-11-09T19:57:12.933831Z"
    }
   },
   "id": "a25d42a5157d1200"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__len__()\n",
    "dataset.__getitem__(1)['latent_text'].size() #dict_keys(['pointcloud', 'latent_text', 'id', 'shift', 'scale'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T19:57:22.161471Z",
     "start_time": "2023-11-09T19:57:22.152231Z"
    }
   },
   "id": "fa979015e7756782"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load back from Checkpoint"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90f8a068b825aba"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from utils.misc import CheckpointManager\n",
    "\n",
    "log_dir = './logs_gen/GEN_2023_11_04__16_12_14'\n",
    "\n",
    "ckpt_mgr = CheckpointManager(log_dir)\n",
    "\n",
    "state = ckpt_mgr.load_best()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T19:57:22.331355Z",
     "start_time": "2023-11-09T19:57:22.159292Z"
    }
   },
   "id": "a825af2fe6d20773"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add text conditioning to the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff8804804ab87b34"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from models.common import reparameterize_gaussian, gaussian_entropy, standard_normal_logprob, truncated_normal_\n",
    "from models.diffusion import DiffusionPoint, PointwiseNet, VarianceSchedule\n",
    "from models.flow import build_latent_flow\n",
    "from models.encoders import PointNetEncoder\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class FlowVAE(Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.encoder = PointNetEncoder(args.latent_dim)\n",
    "        self.flow = build_latent_flow(args, args.latent_text_dim)\n",
    "        self.diffusion = DiffusionPoint(\n",
    "            net = PointwiseNet(point_dim=3, context_dim=args.latent_dim, residual=args.residual),\n",
    "            var_sched = VarianceSchedule(\n",
    "                num_steps=args.num_steps,\n",
    "                beta_1=args.beta_1,\n",
    "                beta_T=args.beta_T,\n",
    "                mode=args.sched_mode\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_loss(self, x, encoded_text, kl_weight, writer=None, it=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x:  Input point clouds, (B, N, d).\n",
    "        \"\"\"\n",
    "        batch_size, _, _ = x.size()\n",
    "        # print(x.size())\n",
    "        z_mu, z_sigma = self.encoder(x)\n",
    "        #print(z_mu.size()) #torch.Size([64, 256]) [batch_size, latent_dim]\n",
    "        #print(z_sigma.size()) #torch.Size([64, 256]) [batch_size, latent_dim]\n",
    "        \n",
    "        z = reparameterize_gaussian(mean=z_mu, logvar=z_sigma)  # (B, F)\n",
    "        print(z.size()) # [batch_size * latent_dim]\n",
    "\n",
    "        # H[Q(z|X)]\n",
    "        entropy = gaussian_entropy(logvar=z_sigma)      # (B, )\n",
    "        \n",
    "        # Condition the latent z vector by custom encoded_text\n",
    "        conditioned_z = torch.cat((z , encoded_text), 1)\n",
    "\n",
    "        # P(z), Prior probability, parameterized by the flow: z -> w.\n",
    "        w, delta_log_pw = self.flow(conditioned_z, torch.zeros([batch_size, 1]).to(z), reverse=False)\n",
    "        log_pw = standard_normal_logprob(w).view(batch_size, -1).sum(dim=1, keepdim=True)   # (B, 1)\n",
    "        log_pz = log_pw - delta_log_pw.view(batch_size, 1)  # (B, 1)\n",
    "\n",
    "        # Negative ELBO of P(X|z)\n",
    "        neg_elbo = self.diffusion.get_loss(x, z)\n",
    "\n",
    "        # Loss\n",
    "        loss_entropy = -entropy.mean()\n",
    "        loss_prior = -log_pz.mean()\n",
    "        loss_recons = neg_elbo\n",
    "        loss = kl_weight*(loss_entropy + loss_prior) + neg_elbo\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('train/loss_entropy', loss_entropy, it)\n",
    "            writer.add_scalar('train/loss_prior', loss_prior, it)\n",
    "            writer.add_scalar('train/loss_recons', loss_recons, it)\n",
    "            writer.add_scalar('train/z_mean', z_mu.mean(), it)\n",
    "            writer.add_scalar('train/z_mag', z_mu.abs().max(), it)\n",
    "            writer.add_scalar('train/z_var', (0.5*z_sigma).exp().mean(), it)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def sample(self, w, num_points, flexibility, truncate_std=None):\n",
    "        batch_size, _ = w.size()\n",
    "        if truncate_std is not None:\n",
    "            w = truncated_normal_(w, mean=0, std=1, trunc_std=truncate_std)\n",
    "        # Reverse: z <- w.\n",
    "        z = self.flow(w, reverse=True).view(batch_size, -1)\n",
    "        samples = self.diffusion.sample(num_points, context=z, flexibility=flexibility)\n",
    "        return samples, z\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T20:07:09.901857Z",
     "start_time": "2023-11-09T20:07:09.898188Z"
    }
   },
   "id": "1647a37d95d1b890"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils.data import get_data_iterator\n",
    "\n",
    "device = 'cpu'\n",
    "args = state['args']\n",
    "args.latent_text_dim = 128\n",
    "args.train_batch_size = 69\n",
    "\n",
    "train_iter = get_data_iterator(DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.train_batch_size,\n",
    "    num_workers=0,\n",
    "))\n",
    "\n",
    "model = FlowVAE(args).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T20:07:12.014904Z",
     "start_time": "2023-11-09T20:07:11.980903Z"
    }
   },
   "id": "af6dd128542fe998"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(136.3206, grad_fn=<AddBackward0>)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.misc import BlackHole\n",
    "\n",
    "batch = next(train_iter)\n",
    "writer = BlackHole()\n",
    "\n",
    "#batch.keys() # dict_keys(['pointcloud', 'latent_text', 'id', 'shift', 'scale'])\n",
    "\n",
    "x = batch['pointcloud'].to(device)\n",
    "encoded_text = batch['latent_text'].to(device)\n",
    "it = 0\n",
    "\n",
    "loss = model.get_loss(x, encoded_text, kl_weight=args.kl_weight, writer=writer, it=it)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T20:07:38.172239Z",
     "start_time": "2023-11-09T20:07:29.577902Z"
    }
   },
   "id": "6862fa83aaef9371"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(136.3206, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T20:07:38.177390Z",
     "start_time": "2023-11-09T20:07:38.171289Z"
    }
   },
   "id": "bf72f61966503ad8"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "128"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)['latent_text'].shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T20:10:42.069174Z",
     "start_time": "2023-11-09T20:10:42.058105Z"
    }
   },
   "id": "930b914312da1e39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn([1, args.latent_dim]).to(device)\n",
    "    x = model.sample(z, args.sample_num_points, flexibility=args.flexibility)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-09T19:57:28.161585Z"
    }
   },
   "id": "84afab64598300c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-09T19:57:28.163355Z"
    }
   },
   "id": "fc66aa42809501a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text to latent"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2714e1b5e0ca082"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kostyalbalint/Documents/Egyetem/7.Felev/Szakdolgozat/diffusion-point-cloud/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "\n",
    "def tokenize_sentences(sentence):\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"setu4993/LEALLA-small\")\n",
    "    tokenizer_model = BertModel.from_pretrained(\"setu4993/LEALLA-small\").to('mps')\n",
    "    tokenizer_model = tokenizer_model.eval()\n",
    "    english_inputs = tokenizer([sentence], return_tensors=\"pt\", padding=True, max_length=512, truncation=True).to('mps')\n",
    "    with torch.no_grad():\n",
    "        english_outputs = tokenizer_model(**english_inputs).pooler_output\n",
    "\n",
    "    return english_outputs.cpu().numpy()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T20:39:54.107789Z",
     "start_time": "2023-11-09T20:39:53.214684Z"
    }
   },
   "id": "6c1ac8d37610bad8"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "t = tokenize_sentences(\"Table\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T20:47:26.640410Z",
     "start_time": "2023-11-09T20:47:24.862253Z"
    }
   },
   "id": "f6ed55be3882de7c"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2]])"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(t.shape)\n",
    "\n",
    "np.resize(t, (5, t.shape[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T20:53:51.948359Z",
     "start_time": "2023-11-09T20:53:51.944933Z"
    }
   },
   "id": "c71a637e023605fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
